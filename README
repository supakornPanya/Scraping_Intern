# ğŸ“ CEDT Intern Position Finder

Web scraping + data analysis app for finding internship positions.  
**Run:** `streamlit run main.py`

## ğŸš€ Quick Start

```bash
git clone https://github.com/supakornPanya/Scraping_Intern.git
cd Scraping_Intern
pip install -r requirements.txt
echo "COOKIE=athena_session=YOUR_COOKIE_VALUE" > .env
```

## ğŸ“– Usage (3 Tabs)

| Tab | Purpose | Output |
|-----|---------|--------|
| **1ï¸âƒ£ Search** | Scrape via Paginated/Detail API | CSV files |
| **2ï¸âƒ£ Visualize** | Merge CSVs, view stats & charts | Merged data + insights |
| **3ï¸âƒ£ Bookmark** | Filter & auto-bookmark positions | bookmark_log.csv |

## ğŸ“¡ API Endpoints

```
GET  /api/sessions/5/openings?page={p}&limit=20    (Paginated)
GET  /api/sessions/5/openings/{id}                 (Detail)
POST /api/sessions/5/openings/{id}/bookmark        (Bookmark)
```

## ğŸ“ Project Structure

```
â”œâ”€â”€ main.py                 # Streamlit app
â”œâ”€â”€ .env                    # Cookie
â”œâ”€â”€ Helper/
â”‚   â”œâ”€â”€ scraping_Paginated.py
â”‚   â”œâ”€â”€ scraping_Detail.py
â”‚   â”œâ”€â”€ Visualize.py
â”‚   â””â”€â”€ bookmark.py
â””â”€â”€ logs/                   # app.log, error.log
```

## ğŸ’¡ Quick Workflows

**Find Best Positions:**
```
1. Tab 1 â†’ Scrape pages 1-16
2. Tab 2 â†’ Merge CSV â†’ View stats
3. Tab 3 â†’ Filter: 250-400à¸¿, ratio<2, Hybrid
4. Tab 3 â†’ Bookmark All
```

**Compare Data Sources:**
```
1. Tab 1 â†’ Scrape paginated API
2. Tab 1 â†’ Scrape detail API (different IDs)
3. Tab 2 â†’ Select both CSVs â†’ Merge
```

## ğŸ› ï¸ Troubleshooting

| Issue | Fix |
|-------|-----|
| Cookie error | Update `.env` with fresh token from login |
| No CSV files | Run scraping in Tab 1 first |
| Slow scraping | Reduce page/ID range |

## ğŸ“Š Key Features

âœ… Scrape 300+ positions | âœ… Auto-merge & deduplicate | âœ… Statistical analysis  
âœ… Interactive filters | âœ… One-click bookmarking | âœ… Progress tracking

**License:** MIT | **Last Updated:** Dec 14, 2025